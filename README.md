# 大模型性能与安全观测平台

## 项目简介

OpenTest大模型全维观测平台是一套针对大语言模型(LLM)的综合性能与安全评测系统。该平台提供从**题库生成**、**题库变形**、**模型测试**到**结果分析报告**的一站式自动化流程，可多维度量化大型语言模型在不同领域和难度下的表现。平台旨在帮助企业和研究者客观评估模型能力，发现模型优劣，辅助模型优化迭代和选型决策。用户既可以通过友好的Web界面使用平台完整功能，也可以借助SDK或开放API将评测能力集成到自己的应用中。

## 主要功能

- **题库生成：** 支持按照指定领域维度、题型和难度批量生成测试题库。内置多种题型（选择、填空、判断、简答等）模板，支持一次性生成500+道题目，并确保题目内容符合预设要求和安全规范。
- **题库变形：** 提供多种题目变形策略（如同义词替换、句式转换、难度调整、选项重组等），可对已有题库进行语义等价的改写扩充。支持批量变形操作，并实时预览变形结果，以丰富测试题库、提高测试覆盖面。
- **模型评估：** 集成国内外主流大语言模型接口（如DeepSeek、OpenAI GPT-4等），对模型逐题生成回答并进行评分。评估指标涵盖正确率、相关性、语言流畅度、安全合规等，多指标综合计算模型得分。支持同时对多个模型进行评测以比较性能。
- **结果分析与报告：** 自动生成结构化的评测报告，包含模型总体得分、各领域细分得分、历史成绩趋势和改进建议等。报告以JSON和PDF两种格式输出，前端提供丰富的可视化组件（柱状图、雷达图、热力图等）展示模型能力差异，并支持模型间横向比较分析。用户可下载报告用于线下存档或分享。

## 系统架构

平台采用前后端分离的三层架构：前端展示层、后端服务层和数据存储层。前端使用 **Vue3 + Element Plus** 框架构建单页应用，通过 **Axios** 与后端交互，并使用 **ECharts** 实现图表可视化。后端基于 **Flask** 实现RESTful API服务，集成 **Flask-CORS** 处理跨域。核心后端模块包括：题库生成与变形模块、模型管理与评估模块、报告生成模块、文件管理模块等。后端通过调用第三方模型API（如OpenAI、DeepSeek等，需要相应API Key）完成问答生成和评分计算。数据存储层采用本地文件系统保存题库、评测结果JSON和报告PDF等文件（支持扩展为数据库存储，.env中预留了PostgreSQL配置）。任务队列使用 **Redis** 实现（可选）以支持评测任务的异步处理和状态维护，从而保证在批量评测场景下系统的稳定性和可扩展性。

整体架构如下：前端发送请求 -> 后端处理调用模型 -> 保存结果并生成报告 -> 前端可视化展示/下载报告。

通过这样的架构设计，平台既保证了各模块松耦合和清晰的分工，又为后续功能拓展（新增模型接入、分布式部署等）奠定了基础。

## 环境要求

- **操作系统：** Windows 10/11 或 Linux （部署用于开发测试，服务器部署可选Linux）
- **Python环境：** Python 3.8 及以上版本
- **Node.js环境：** Node.js 16+（用于前端构建与运行，如仅使用后端API和SDK可选装）
- **依赖组件：** 后端需要安装 `Flask`、`requests`、`pandas`、`numpy`、`reportlab` 等（详见 `requirements.txt`），以及 Redis 数据库（可选，用于任务队列功能）。前端需要安装 Vue3 脚手架（Vue CLI 或 Vite）、Node.js及相关依赖。

## 安装部署步骤

1. **克隆代码库：** 下载或克隆本项目源码到本地。

   ```bash
   git clone https://github.com/xkingxking/mine.git
   cd mine
   ```

2. **配置环境：** 将仓库根目录下的 `.env.example` 文件复制一份命名为 `.env`。根据实际情况编辑 `.env` 文件，如各大模型的 API Key 等。若无特殊需求，保持默认配置即可。

3. **安装后端依赖：** 确保已安装符合要求的 Python 环境，建议使用虚拟环境（`venv` 或 Anaconda）。进入项目根目录，执行：

   ```bash
   pip install -r requirements.txt
   ```

   这将安装 Flask、requests 等所有后端所需依赖库。

4. **安装前端依赖：** 如需运行前端界面，需安装 Node.js 与包管理器 `npm`。进入 `frontend` 子目录，执行：

   ```bash
   npm install
   npm run build   # 构建前端静态文件
   ```

   构建完成后，生成的静态前端文件将在 `frontend/dist` 目录下，可由后端提供静态服务或使用`npm run dev`进行开发调试。

5. **初始化运行：** 确保 Redis 服务已启动（如果启用了任务队列功能，可选）。然后在项目根目录下启动后端服务：

   ```bash
   python -m app.api.api
   ```

   若启动成功，控制台将提示 Flask 正在监听默认端口（5000）。在生产部署中，可使用 Gunicorn 或其他WSGI容器托管 Flask 应用。

## 使用说明

- **通过浏览器使用：** 若前端已构建并正确部署，打开浏览器访问 `http://localhost:5000`（假设后端运行在本机5000端口）。进入平台后，可在界面上按顺序完成评测流程：在“题库管理”模块设置参数生成题库 → 在“题库变形”模块选择已有题库执行变形→ 在“模型评估”模块选择模型对题库进行测试 → 在“报告管理”模块查看下载评测报告，并可使用“模型对比”功能横向比较多个模型。平台界面提供直观的操作向导和状态提示，适合一般用户使用。

- **通过SDK调用：** 开发者也可以使用平台提供的 Python SDK 进行程序化调用。在确保后端服务已启动的前提下，复制 `SDK/mine_sdk.py` 文件到自己的项目中，然后：

  ```python
  from mine_sdk import MineSDK  
  sdk = MineSDK(base_url="http://localhost:5000")  # 初始化SDK实例，指向后端服务地址（可以指向部署好的web服务：http://opentest-llm.com)
  ```

  之后可以调用 `sdk.list_question_banks()` 可获取当前题库列表；使用 `sdk.create_question_bank(name, dimensions, difficulties, count)` 创建新题库；后续可调用 `sdk.evaluate_model(model_name, questions, dataset_path)` 提交模型评估任务，并通过 `sdk.download_report(filename)` 下载评估报告等。
  SDK 封装了完整的API调用流程，更多示例参见项目中的 `SDK/sdk_use.py` 及`SDK/README.md`。通过SDK，用户能够在不打开浏览器的情况下，在任意Python环境下远程调用评测服务，实现与自身业务系统的集成。

- **通过Saas调用**：平台已通过腾讯云服务器进行了互联网部署，用户可以通过在浏览器中访问域名：http://opentest-llm.com，或公网地址：http://119.29.147.62/ 对平台进行Saas调用。